{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a1b0d07-834d-4433-b546-27eb1319a00c",
   "metadata": {},
   "source": [
    "## Install `torch`\n",
    "link: https://pytorch.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7486a9b0-2fbb-4c07-b670-15d885eea030",
   "metadata": {},
   "source": [
    "**As a side note:** <br>\n",
    "You may want to check out the following cloud platforms:\n",
    "* AWS\n",
    "* MS Azure\n",
    "* GCP\n",
    "* Lightning AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4322f11-4666-43c0-bfb1-5e78086f03be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47d67699-8a4c-4851-a826-ba072fee6729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8252b1-cbba-4a9c-ba93-5f145393ed2b",
   "metadata": {},
   "source": [
    "It returns `False` in my mac as my GPU is not supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "926356cb-b687-44ed-a2cf-cc0d685ed9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# check if Apple Silicon Chip can accelerate PyTorch's computation\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c51d4b-0447-42f5-86f6-4e99d75fca1b",
   "metadata": {},
   "source": [
    "## Three main components of the PyTorch library\n",
    "\n",
    "(1) Tensors <br>\n",
    "(2) Automatic differentiation <br>\n",
    "(3) Deep Learning layers / pre-built model / etc <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45161b25-c13f-425c-bc5a-aff093fb3113",
   "metadata": {},
   "source": [
    "## Tensors vs Numpy ndarray\n",
    "\n",
    "Tensors with rank n is like n-dim numpy array. Tensors have more features and they are supported for GPU's computation. Tensor with rank 0 is just a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e4bf2b-c912-4cf4-9942-81f2d1814e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "data: \n",
      " 1\n",
      "type of out: \n",
      " <class 'torch.Tensor'>\n",
      "out: \n",
      " tensor(1)\n",
      "==================================================\n",
      "data: \n",
      " [2, 3, 4]\n",
      "type of out: \n",
      " <class 'torch.Tensor'>\n",
      "out: \n",
      " tensor([2, 3, 4])\n",
      "==================================================\n",
      "data: \n",
      " [[ 3  5]\n",
      " [ 0 -1]]\n",
      "type of out: \n",
      " <class 'torch.Tensor'>\n",
      "out: \n",
      " tensor([[ 3,  5],\n",
      "        [ 0, -1]])\n"
     ]
    }
   ],
   "source": [
    "# let's create some tensors!\n",
    "lst_of_data = [\n",
    "    1,\n",
    "    [2, 3, 4],\n",
    "    np.array([[3, 5],[0, -1]]),\n",
    "]\n",
    "\n",
    "for data in lst_of_data:\n",
    "    print('=' * 50)\n",
    "    print('data: \\n', data)\n",
    "    out = torch.tensor(data)\n",
    "    print('type of out: \\n', type(out))\n",
    "    print('out: \\n', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f86a7dc-8169-4576-8e7b-ef450ddf52fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  5],\n",
       "        [ 0, -1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[3, 5],[0, -1]])\n",
    "a = torch.tensor(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a852313-c664-4c17-ace5-9b36a585fdef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bccc681f-e4ce-407a-b07a-1b686bfb0f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3],\n",
       "        [ 5],\n",
       "        [ 0],\n",
       "        [-1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(4, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a6058e7-998d-4138-bf58-574c7aa67328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14d57a62-ca0c-4118-9b48-9ceccf883c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  5],\n",
       "        [ 0, -1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3494b5b8-c71a-4350-bb6e-e9266ee34c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  0],\n",
       "        [ 5, -1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6b436a-11f8-4afd-b1bc-f8bf4fb023a9",
   "metadata": {},
   "source": [
    "## Backward propagation in computation graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b30c041-807d-46f0-baac-5db71cd8499a",
   "metadata": {},
   "source": [
    "A computation graph is a graph that represents a computation. For instance, y = a * x + 3 can be shown like:\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "\n",
    "A[a] --> C[*]\n",
    "B[x] --> C\n",
    "C --> C2[v]\n",
    "C2 --> E[+]\n",
    "D[3] --> E\n",
    "E --> F[y]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d81b7-f618-4f18-b53c-d7d34904d641",
   "metadata": {},
   "source": [
    "To find out the impact of change in input on output, we can take the derivative $\\frac{\\partial{y}}{\\partial{x}}$. According to graph, the variable `y` depends on `v`, and `v` depends on `x`. So, we can use the chain rule:\n",
    "\n",
    "$\\frac{\\partial{y}}{\\partial{x}} = \\frac{\\partial{y}}{\\partial{v}} \\times \\frac{\\partial{v}}{\\partial{x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b874c73-7a7a-49df-b786-2b7153d91790",
   "metadata": {},
   "source": [
    "So, if an algorithm keeps updating those derivatives, I can quickly compute the new value, and plug them into the equation above to compute $\\frac{\\partial{y}}{\\partial{x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260239ad-50d5-4017-b722-6ad7fc8eb70e",
   "metadata": {},
   "source": [
    "**NOTE:** For more details, you may check out the Andrew NG's Deep Learning course in coursera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2840a310-bba8-4728-969a-6c5535388f3f",
   "metadata": {},
   "source": [
    "`PyTorch` has a high-level method `.backward` which does all those good stuff for us! The author showed it for a simple logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7072b5f2-2e1e-4e9d-a0fd-f14df6f3692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "import torch.nn.functional as F\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z)\n",
    "loss = F.binary_cross_entropy(a, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24749fd8-16eb-40b0-bd9e-9daaebbf3b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of w1:  tensor([-0.0898])\n",
      "gradient of b:  tensor([-0.0817])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()  # compute gradient of all leaf nodes that require_grad==True (?)\n",
    "\n",
    "print('gradient of w1: ', w1.grad)\n",
    "print('gradient of b: ', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c6df82-a14e-43ad-914d-99d751a31b5d",
   "metadata": {},
   "source": [
    "## Implementing multi-layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42ea1dc5-a8c4-40ce-98ed-fc8bfbef02de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_inputs : int\n",
    "            The number of nodes in the input layer\n",
    "\n",
    "        n_outputs : int\n",
    "            The number of nodes in the output layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            # 1st hidden layer\n",
    "            torch.nn.Linear(n_inputs, 30),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(30, 20),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # output layer\n",
    "            torch.nn.Linear(20, n_outputs),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):  \n",
    "        # what does this do?! \n",
    "        # Are we overwriting `forward` method?\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5945e437-57ef-4840-ba4e-3ffeec652332",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "N_INPUTS = 50\n",
    "N_OUTPUTS = 3\n",
    "model = NeuralNetwork(N_INPUTS, N_OUTPUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abb72d7c-f378-46fd-9aea-40728e419302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e7560dd-b967-4d0b-9d16-d3fb2d861659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Parameter containing:\n",
      "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
      "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
      "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
      "        ...,\n",
      "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
      "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
      "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
      "       requires_grad=True)\n",
      "shape:  torch.Size([30, 50])\n",
      "1500\n",
      "==================================================\n",
      "Parameter containing:\n",
      "tensor([-0.1250,  0.0513,  0.0366,  0.0075,  0.0509,  0.0545, -0.0393,  0.0924,\n",
      "        -0.1412, -0.1232, -0.1063,  0.0081, -0.1249,  0.0101, -0.0019, -0.1298,\n",
      "         0.1388, -0.0330,  0.1017,  0.1247, -0.0554, -0.0417,  0.1388,  0.0159,\n",
      "         0.1215,  0.0385,  0.0769, -0.1224, -0.0279,  0.0991],\n",
      "       requires_grad=True)\n",
      "shape:  torch.Size([30])\n",
      "30\n",
      "==================================================\n",
      "Parameter containing:\n",
      "tensor([[-1.0154e-02, -1.5861e-01,  1.9066e-02,  1.6987e-02, -1.7074e-02,\n",
      "          9.4865e-02,  7.1011e-02, -9.2097e-02, -3.2471e-02, -1.1231e-01,\n",
      "          8.6465e-02, -1.0711e-01, -9.6259e-02, -4.4155e-03,  1.2285e-01,\n",
      "          1.3708e-01, -1.5861e-01, -1.7688e-01,  1.3864e-01, -9.0875e-02,\n",
      "         -1.6941e-01, -1.3771e-02,  1.3315e-01, -2.3373e-02,  4.0696e-02,\n",
      "         -1.7768e-01,  1.5031e-01,  1.1774e-01,  1.2701e-01,  9.4861e-02],\n",
      "        [-9.3387e-02, -1.5303e-01,  6.9710e-04,  9.7921e-02, -7.9413e-02,\n",
      "          4.5254e-02, -1.4399e-02, -1.6972e-02,  4.7111e-02,  1.3326e-01,\n",
      "          3.8191e-02,  1.3138e-02,  2.9559e-02,  3.3591e-02, -5.9173e-02,\n",
      "         -2.0820e-02,  4.4497e-02, -1.4383e-01,  1.2064e-04, -1.6885e-01,\n",
      "         -1.6863e-01,  3.2975e-03,  2.5393e-02,  3.4459e-02, -6.7400e-02,\n",
      "         -2.0042e-02, -2.0234e-02, -6.0862e-03, -1.1799e-01,  3.9853e-02],\n",
      "        [ 5.6563e-02,  5.2087e-03, -1.4817e-02,  8.8404e-02,  1.2358e-01,\n",
      "         -2.2885e-03, -1.3856e-01, -1.2713e-01, -3.5647e-03, -1.6688e-01,\n",
      "         -1.3804e-01, -4.4271e-02, -7.3010e-02, -2.3431e-02,  4.4115e-02,\n",
      "          1.2497e-01,  1.8171e-01, -7.3016e-02, -1.7838e-01,  1.2122e-01,\n",
      "          1.5906e-01,  1.7420e-01,  4.1410e-02,  1.1633e-01,  4.3209e-04,\n",
      "         -1.2939e-01, -1.4691e-01, -1.0952e-01,  2.7672e-02, -4.6244e-02],\n",
      "        [ 4.6685e-02,  6.4175e-02, -1.1571e-01,  1.4561e-02,  1.7691e-01,\n",
      "          1.2027e-01, -2.0332e-02, -3.8157e-02, -7.1255e-02,  5.9831e-02,\n",
      "         -1.1019e-01,  1.8084e-01,  2.5788e-03, -1.1797e-01, -8.8593e-02,\n",
      "         -7.0314e-02, -1.4497e-01, -5.7067e-02, -1.7528e-01,  6.1810e-02,\n",
      "         -5.9462e-02,  4.4371e-02, -8.9423e-02, -8.2602e-02, -8.9245e-02,\n",
      "         -1.6231e-03, -1.3898e-01, -1.6236e-01,  3.7768e-02, -1.5703e-01],\n",
      "        [ 1.4778e-01,  1.3778e-01,  1.0478e-01,  1.2295e-01, -1.0692e-01,\n",
      "          2.7535e-02,  3.9353e-02,  1.5920e-01, -1.6470e-01, -5.2501e-02,\n",
      "          9.2477e-02,  9.6969e-02, -1.7690e-01,  1.6811e-01,  1.2099e-01,\n",
      "          1.5483e-01, -6.6973e-02, -9.2702e-02,  5.0032e-02,  1.3386e-01,\n",
      "          1.5885e-01, -1.1990e-01,  1.1666e-01, -2.9270e-02, -8.5924e-02,\n",
      "          1.0728e-01,  8.5550e-02,  4.0957e-02, -1.1469e-01,  1.7155e-01],\n",
      "        [-3.1611e-02,  5.5533e-03, -5.7722e-02, -1.5423e-01, -3.2761e-02,\n",
      "          3.6143e-02,  1.4452e-02, -4.2721e-02,  1.2813e-01, -2.6680e-02,\n",
      "         -1.3917e-01, -7.3217e-02, -4.3458e-02,  1.1988e-01,  1.4263e-01,\n",
      "          6.3297e-02,  1.7623e-01,  1.7438e-01,  4.1272e-02,  1.2515e-01,\n",
      "          7.8728e-02,  2.6415e-02,  1.6633e-01, -4.7062e-02, -1.4514e-01,\n",
      "         -1.6136e-01,  1.1267e-01, -1.2828e-01,  7.8414e-02, -1.0403e-02],\n",
      "        [-1.7202e-01, -5.8844e-02,  6.5513e-02,  5.7691e-04,  7.1033e-02,\n",
      "         -7.0540e-02, -8.3165e-02, -1.3339e-02, -1.6084e-01, -3.7179e-02,\n",
      "          1.0081e-02,  7.4172e-02, -5.6515e-02,  1.1535e-01, -4.6388e-03,\n",
      "         -4.7255e-02, -5.0935e-02,  1.2562e-01,  1.1003e-02, -1.0269e-01,\n",
      "         -1.0559e-01, -6.5035e-04,  4.2604e-02, -1.1221e-01, -1.6542e-01,\n",
      "          7.5141e-02,  5.5210e-02, -1.0214e-01, -9.9271e-02,  1.4185e-01],\n",
      "        [-1.1977e-01,  1.2830e-01, -1.1554e-01,  6.9452e-02, -9.3266e-03,\n",
      "          6.8769e-02,  8.1484e-02, -7.9705e-02, -1.6156e-01, -2.9207e-03,\n",
      "          3.1119e-02, -2.0498e-02,  6.6412e-02,  1.8065e-01, -7.4688e-02,\n",
      "          6.0297e-02,  5.7398e-02,  1.0436e-01, -7.7763e-02, -1.7681e-01,\n",
      "         -1.0666e-01,  1.2395e-02, -1.8109e-01,  1.5387e-01,  1.4700e-01,\n",
      "          4.2823e-02, -1.3201e-01, -1.7369e-01,  1.0825e-01, -1.2057e-01],\n",
      "        [ 6.2061e-02,  1.7306e-01, -1.0462e-01,  6.9222e-02, -9.5118e-03,\n",
      "          7.0322e-02,  8.6850e-02, -3.9505e-03,  3.2307e-02, -3.9305e-02,\n",
      "          4.9267e-02,  4.2660e-02,  1.1491e-01,  1.5994e-01,  4.9567e-02,\n",
      "          1.4213e-01, -1.7885e-01, -8.1386e-02, -1.6261e-01, -7.4337e-02,\n",
      "          1.4070e-01, -1.6133e-01,  1.0495e-01, -5.6505e-02,  8.8895e-03,\n",
      "         -1.1880e-01, -1.0216e-01,  1.6443e-02,  1.8018e-02, -1.5349e-01],\n",
      "        [-5.4879e-03, -6.9545e-02, -1.2516e-01, -1.5054e-01,  5.1067e-02,\n",
      "          2.3657e-02,  1.0642e-01,  1.4157e-01,  6.5826e-02, -6.0076e-02,\n",
      "          1.1329e-01,  5.6642e-02,  1.3649e-01,  9.6732e-02,  4.1889e-02,\n",
      "          9.0102e-02, -1.7953e-01,  9.4747e-02,  1.2196e-01, -2.4367e-02,\n",
      "         -1.5831e-01,  1.1654e-01,  9.3345e-03, -5.1091e-02, -2.8094e-02,\n",
      "          1.4220e-01, -6.0935e-02,  1.7947e-01,  1.3134e-01,  6.9098e-02],\n",
      "        [ 9.5976e-02, -1.2119e-01, -1.5018e-01,  1.3293e-01,  1.1696e-01,\n",
      "         -1.4044e-01, -1.1161e-01,  7.6049e-02,  5.9951e-02,  1.1501e-01,\n",
      "         -1.2624e-01,  9.0305e-02,  1.5189e-02, -1.3900e-01,  1.8119e-01,\n",
      "         -3.3564e-02, -7.9913e-02, -1.5007e-01, -6.2586e-02, -8.8812e-02,\n",
      "         -6.6300e-03, -1.3452e-01, -1.7013e-01, -1.7279e-01, -1.0307e-01,\n",
      "         -6.6781e-02,  2.3535e-02,  4.4560e-02,  1.0916e-01, -1.2035e-01],\n",
      "        [ 1.1683e-01,  1.1265e-01, -1.5032e-01, -1.2258e-01, -9.9277e-02,\n",
      "         -8.5493e-03, -1.7132e-01,  2.6558e-02,  7.4480e-02,  1.3092e-01,\n",
      "          1.1298e-01, -9.1413e-02, -1.2121e-01,  1.0977e-01, -1.5265e-02,\n",
      "         -1.0690e-01,  1.0925e-01,  9.0309e-02,  1.0382e-01, -8.3814e-02,\n",
      "         -2.9132e-02,  1.1034e-01, -1.8141e-01, -1.7327e-01, -1.2493e-01,\n",
      "          8.6094e-02, -1.7258e-01,  7.4506e-02, -1.1579e-01, -5.9486e-02],\n",
      "        [-6.1575e-02,  1.0901e-01,  1.5457e-01,  1.0825e-01, -1.2382e-01,\n",
      "         -2.0479e-02,  9.7674e-02,  1.0308e-01, -8.3447e-02,  8.9352e-02,\n",
      "         -6.6935e-03, -1.4226e-01,  8.3289e-02,  3.7189e-03,  8.2901e-02,\n",
      "          1.3366e-01,  1.0924e-01,  4.2660e-02, -1.2721e-02, -3.9557e-02,\n",
      "         -1.2502e-01, -1.2765e-01,  1.7830e-01, -3.1311e-02,  8.6929e-02,\n",
      "         -3.5704e-02,  1.2051e-01, -4.3428e-02, -1.2294e-01,  1.3583e-01],\n",
      "        [-8.7338e-02,  4.5554e-02, -3.7694e-02,  1.8229e-01,  3.7297e-02,\n",
      "         -4.4091e-02,  5.5953e-02,  1.4041e-01, -1.2216e-01,  1.1264e-01,\n",
      "          1.8199e-01, -7.7902e-02, -5.3841e-02, -3.3260e-02,  1.8807e-02,\n",
      "         -9.2009e-02, -1.6190e-01,  7.0638e-02,  3.3134e-02, -8.7749e-02,\n",
      "          6.2125e-02,  1.1452e-01,  2.0772e-02,  7.0628e-03,  7.5365e-02,\n",
      "         -1.6139e-01, -1.2418e-01,  4.3718e-02, -1.0107e-01,  1.6968e-01],\n",
      "        [-5.8873e-02, -1.4804e-01, -9.2603e-02, -1.9626e-02, -8.7975e-02,\n",
      "          7.4040e-02,  2.2591e-02, -1.2827e-01,  3.3492e-02,  1.3154e-01,\n",
      "          5.9562e-03, -1.7606e-01,  7.4319e-02, -1.7905e-01, -6.0711e-02,\n",
      "          6.7206e-02, -1.6789e-01,  5.3757e-02,  9.8079e-02,  1.5656e-01,\n",
      "          6.6243e-02, -1.6121e-02,  6.7246e-02, -1.1196e-01, -3.3648e-02,\n",
      "         -1.1629e-01, -6.5418e-02, -1.6427e-01, -1.0178e-01, -1.6825e-02],\n",
      "        [ 4.3667e-02, -1.0633e-01,  1.6324e-01,  1.1975e-01,  1.4478e-01,\n",
      "         -1.7948e-02, -4.1938e-02, -1.2280e-01, -6.3642e-02, -3.0262e-02,\n",
      "          6.7827e-02, -6.4390e-02,  1.3669e-01,  1.0732e-01,  1.3488e-01,\n",
      "          7.2901e-02, -1.5806e-01,  1.6297e-01, -2.5397e-03,  1.7322e-01,\n",
      "          1.6979e-01,  3.2193e-02,  8.4038e-02, -1.0434e-01,  1.6646e-01,\n",
      "         -1.5681e-01,  1.2339e-01, -6.7869e-02,  1.7659e-01,  2.9252e-04],\n",
      "        [-9.9509e-02,  7.5632e-02,  2.5662e-02,  1.0011e-01, -1.0985e-01,\n",
      "         -8.1406e-02, -5.7754e-03, -5.0705e-02,  1.2672e-01,  6.5627e-02,\n",
      "         -3.9199e-02,  1.0205e-01,  1.7768e-01, -8.7478e-02,  1.1971e-01,\n",
      "         -1.1522e-01, -1.2282e-01, -1.6962e-01,  2.4679e-02,  1.4132e-01,\n",
      "         -1.7488e-01, -1.3757e-01,  1.1106e-01, -9.0416e-02,  9.1762e-02,\n",
      "         -3.0935e-02,  1.3043e-01,  2.6914e-02,  1.3586e-01, -7.7098e-02],\n",
      "        [-6.4123e-02, -7.3031e-02,  1.3940e-01,  1.7168e-02, -1.2868e-01,\n",
      "          9.8793e-02,  1.1433e-01, -1.1358e-01,  8.7645e-03, -1.7513e-01,\n",
      "          2.3224e-02,  6.4231e-02,  1.6339e-01, -5.9178e-02,  6.5853e-02,\n",
      "         -6.1179e-02, -6.4203e-02, -1.6178e-01,  3.8537e-02,  1.2251e-01,\n",
      "         -1.0157e-01,  7.4643e-02, -2.8479e-02, -1.2963e-01,  1.9468e-02,\n",
      "         -1.7792e-01,  9.7872e-02, -4.0340e-02,  1.6815e-01,  6.7399e-03],\n",
      "        [-1.7700e-01,  1.4168e-01, -3.9935e-02, -1.2506e-01,  1.6857e-01,\n",
      "          1.7052e-02,  5.9193e-02, -1.5794e-01, -6.4479e-02,  1.4649e-01,\n",
      "         -1.2420e-01, -1.4749e-01,  1.4326e-01, -3.1483e-02, -1.3886e-02,\n",
      "          1.2950e-01,  5.7224e-02, -9.3365e-02, -4.4930e-02,  1.1780e-01,\n",
      "         -5.8706e-02,  1.4606e-01,  1.6647e-01,  1.0949e-01, -1.5390e-02,\n",
      "         -8.1016e-02, -1.0014e-01, -1.4818e-01,  1.1121e-03,  1.1822e-01],\n",
      "        [ 7.4134e-02,  1.0339e-02, -1.7135e-01,  8.7192e-02, -4.2253e-02,\n",
      "          2.8489e-02, -1.1050e-01,  2.6522e-02,  4.4801e-02, -4.6616e-02,\n",
      "         -6.4001e-02,  5.5335e-02,  2.6179e-02,  1.2655e-01, -1.3652e-01,\n",
      "          1.7902e-01, -1.6983e-01, -1.3433e-01, -3.2193e-02, -1.5858e-01,\n",
      "          1.2315e-01, -3.1973e-02,  7.0793e-03,  5.9091e-02,  2.0649e-02,\n",
      "          1.5582e-01,  1.3103e-01,  6.2131e-02, -1.7515e-01,  6.8660e-02]],\n",
      "       requires_grad=True)\n",
      "shape:  torch.Size([20, 30])\n",
      "600\n",
      "==================================================\n",
      "Parameter containing:\n",
      "tensor([-0.0301,  0.0310,  0.1400, -0.0861, -0.0116, -0.1509,  0.0662, -0.0400,\n",
      "        -0.0674,  0.0101,  0.0086,  0.0333,  0.1693,  0.1364,  0.1509, -0.1043,\n",
      "        -0.1729,  0.1331,  0.1572,  0.1100], requires_grad=True)\n",
      "shape:  torch.Size([20])\n",
      "20\n",
      "==================================================\n",
      "Parameter containing:\n",
      "tensor([[-0.1692, -0.0858,  0.2063, -0.1134, -0.1537,  0.0388,  0.1798, -0.0861,\n",
      "         -0.0695, -0.0770, -0.1165,  0.0031, -0.0258,  0.0932, -0.1665, -0.0642,\n",
      "         -0.2058, -0.2228,  0.0619, -0.0289],\n",
      "        [-0.2011, -0.0067,  0.0681,  0.1624,  0.1344, -0.2128,  0.0601, -0.2104,\n",
      "          0.1657,  0.1458, -0.2062,  0.0016,  0.0697,  0.0092, -0.2019,  0.2094,\n",
      "         -0.1101,  0.0695,  0.1398, -0.2045],\n",
      "        [-0.0663, -0.1034,  0.2191, -0.0906, -0.1899, -0.2023, -0.0230,  0.1957,\n",
      "         -0.1776, -0.1351, -0.2014,  0.2151,  0.1206,  0.0938,  0.1765,  0.0676,\n",
      "          0.1493, -0.0508,  0.0346, -0.1222]], requires_grad=True)\n",
      "shape:  torch.Size([3, 20])\n",
      "60\n",
      "==================================================\n",
      "Parameter containing:\n",
      "tensor([-0.0535,  0.0220, -0.1022], requires_grad=True)\n",
      "shape:  torch.Size([3])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print('=' * 50)\n",
    "    print(p)\n",
    "    print('shape: ', p.shape)\n",
    "    if p.requires_grad:\n",
    "        print(p.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa6ca44f-65a6-4ab1-9748-1eff6e693ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
       "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
       "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
       "        ...,\n",
       "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
       "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
       "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f671dfa-e12c-4081-9b7b-a710a580c2a1",
   "metadata": {},
   "source": [
    "**Note:** <br>\n",
    "$Y = W^{T}X + B$, where: <br>\n",
    "* X (Input): a data point in $R^{m}$ space; `x1, x2, ..., xm`\n",
    "* Y (Output): a data point in $R^{n}$ space; `y1, y2, ..., yn`\n",
    "* W (Weight): The weight which is $m$-by-$n$, So, `W[col1]` represents the weights for the `y1`, and so on.\n",
    "* B (Bias): The bias vector in $R^{n}$ space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c85019ae-18fd-484a-9078-819738a59e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 50])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the layer0's weight again\n",
    "\n",
    "model.layers[0].weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e188bed-8909-4ffa-adcb-3b991eaa7a3e",
   "metadata": {},
   "source": [
    "It is important to note that the shape of the weight tensor above is actually AFTER transpose. In other words, the shape `(30, 50)` is the shape of $W^{T}$.\n",
    "\n",
    "**NOTE:** <br>\n",
    "The `layers[0]` is not the nodes! It is actually those edges that connects two sets of nodes. Recall that the input has 50 elements. And, then, 30 elements are computed. So, that layer0's weight- with shape `(30, 50)`- actually refers to those edges that connect 50-node layer to the next 30-node layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff8bf8a3-faf3-4453-8443-8d74e107e050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "sample = torch.rand((1, 50))\n",
    "out = model(sample)  # should be equivalent to model.forward(x). will check it out shortly\n",
    "print(out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75994110-1237-4e11-9b3e-9c58153b107e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528dcbe9-9983-4707-99fa-d1c3f0ed6153",
   "metadata": {},
   "source": [
    "If we are done with training, and we only need to do inference (prediction), there is no need for backpropagation. All we need now is to just allow the input value goes through all layers so that the output can be computed.So, if the goal is only inference, we can avoid keeping track of gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4e90d46-ad81-4e6e-80d3-c7fae6534958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(sample)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a7046-621c-4222-abc4-9b818f0ad8e8",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "If user wants to get probability of each class, then the output of the model needs to be fed into softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95ddd26c-07dd-4305-8647-7c588bd0637c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax_output:\n",
      " tensor([[0.3113, 0.3934, 0.2952]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(sample)\n",
    "    softmax_output = torch.softmax(out, dim=1)\n",
    "\n",
    "print('softmax_output:\\n', softmax_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c082e112-6779-4e8e-b0b8-4f11a7be59b4",
   "metadata": {},
   "source": [
    "## Setting up Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92abc8f-f46e-49f6-9616-b04de8c6b227",
   "metadata": {},
   "source": [
    "**Simple Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3045a8ca-c2fb-4285-b1b1-72ae88a9d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features: two\n",
    "\n",
    "# train, with fives records\n",
    "X_train = torch.tensor([\n",
    "    [-1.2, 3.1],\n",
    "    [-0.9, 2.9],\n",
    "    [-0.5, 2.6],\n",
    "    [2.3, -1.1],\n",
    "    [2.7, -1.5]\n",
    "])\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
    "\n",
    "# test, with fives records\n",
    "X_test = torch.tensor([\n",
    "    [-0.8, 2.8],\n",
    "    [2.6, -1.6],\n",
    "])\n",
    "y_test = torch.tensor([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678cfed0-61f3-468d-989c-3f61f6fbd481",
   "metadata": {},
   "source": [
    "What are we trying to do here? We want to create a class to help us get samples from our `TRAIN` & `TEST` set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2c6763cc-0ce9-41e7-8fe3-6ea1628f485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "80257670-0c30-42ee-a648-369553347b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        index is the index of an existing sample in X\n",
    "        \"\"\"\n",
    "        return self.features[index], self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea3f52-4771-4364-a3e3-513c244d6771",
   "metadata": {},
   "source": [
    "**Why do we need this class?** <br>\n",
    "\n",
    "Because we will use `DataLoader` to create the iterator to help us iterate through our dataset and get a certain batch of data. The \"data set\" object that is passed as input to the class `DataLoader` needs to support two methods `__getitem__` and `__len__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a3a504f9-f761-4fae-a6b8-1dd9d06253a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ToyDataset(X_train, y_train)\n",
    "test_ds = ToyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "943ecc74-7962-49b3-aebd-1649be33e1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ToyDataset at 0x1431de120>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1803a024-b2d6-4c4f-a033-64d384210f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.5000,  2.6000]), tensor(0))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.__getitem__(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a9c9db-1cce-480b-aa62-2318677ecc8e",
   "metadata": {},
   "source": [
    "## use DataLoader() class to create iterator for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9515ed7b-31ca-452b-927c-5d8a0040bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,  \n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fb9551ec-a866-49ce-a16b-916ba210a3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0, and train data: \n",
      "[tensor([[-1.2000,  3.1000],\n",
      "        [-0.5000,  2.6000]]), tensor([0, 0])]\n",
      "--------------------------------------------------\n",
      "i=1, and train data: \n",
      "[tensor([[ 2.3000, -1.1000],\n",
      "        [-0.9000,  2.9000]]), tensor([1, 0])]\n",
      "--------------------------------------------------\n",
      "i=2, and train data: \n",
      "[tensor([[ 2.7000, -1.5000]]), tensor([1])]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(train_loader):\n",
    "    print(f'i={i}, and train data: \\n{item}')\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7cafa6-e4ce-4121-8eef-8429a72ce5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
