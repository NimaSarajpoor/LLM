{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a43ca4-7b6b-44f8-8e25-b91e4da8cd09",
   "metadata": {},
   "source": [
    "**NOTE:** The following information is based on the book \"Build Large Language Model From Scratch\" By Sebastian Raschka. I am just trying to take notes, explain some stuff further for myself when needed, and do some coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f1d6b42-1bbb-4cd3-b0c1-1d581d6fbd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'core' from '/Users/nimasarajpoor/Desktop/LLM/HOW-IT-WORKS/core.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import core\n",
    "import importlib\n",
    "\n",
    "importlib.reload(core)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe620c2-4db6-433e-9f4b-aefda6557ef0",
   "metadata": {},
   "source": [
    "## What is `word embedding`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9fc5a5-32ea-4f37-b139-dc767de58023",
   "metadata": {},
   "source": [
    "It simply means map a word to a point in n-dim Euclidean space. Let's denote `f` as the word-embedding process. Then:\n",
    "\n",
    "$ v = f(word)$ <br>\n",
    "$ f: text(words) \\rightarrow R^{n}$\n",
    "\n",
    "\n",
    "**Why do we need it?** <br>\n",
    "Because the NN does computation on numerical values. So, need to convert text to some numerical values first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0b3c44-36b9-4587-a05b-85d59a734b1f",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "The function above may not be completely accurate but it gives us the idea about what `word embedding` does. It simply gets a word as input, and returns a n-dim vector as output. This vector is the representation of the word. If there are several words that are close in meaning, their corresponding points in $R^{n}$ space will be close to each other. In fact, if I have two words and want to know their similarity, I can compute the cosine value of the angle between those vectors: <br>\n",
    "\n",
    "$SimilarityScore(word1, word2) = cos(v1, v2) = \\frac{v1.v2}{|v1|.|v2|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a923e57e-080c-4c83-8001-6aa638deaf19",
   "metadata": {},
   "source": [
    "**NOTE: embedding is only for word? RAG!** <br>\n",
    "\n",
    "While word embeddings are the most common form of text embedding, there are also embeddings for sentences, paragraphs, or whole documents. Sentence or paragraph embeddings are popular choices for retrieval-augmented generation (RAG). RAG combines generation (like producing text) with retrieval (like searching an external knowledge base) to pull relevant information when generating text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d823e77-d6c2-4a69-aabe-c264e96e4e9f",
   "metadata": {},
   "source": [
    "## How to obtain those embedding words?\n",
    "\n",
    "There are some already-trained NN that gives us the embedding words (e.g. `word2vec`). For LLM, there is no need to use an extra model for obtaining embedding words. In LLM, the word embedding is computaed as part of the input layer in LLM, and its advantage is that it is optimized during the training process for the domain/task-specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeba1e3-4983-4614-bf98-f14e07df2089",
   "metadata": {},
   "source": [
    "### Step 1. Tokenizing text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c264a8-e2d0-4aea-b66e-5035590a9e0a",
   "metadata": {},
   "source": [
    "So far, we have learned that we need to obtain word embedding, which is achieved by mapping each word to its corresponding point in a $R^{n}$ space. Those vectors can be used as input to LLM. But our data is often a text with several paragraphs / sentences, and not just a list of words. So, our first task is to understand how one can text file and break it down to a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311f61e0-4c83-4878-adb6-2d24cbe0ed36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the-verdict.txt\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    + \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "    + \"the-verdict.txt\"\n",
    ")\n",
    "file_path = \"./data/the-verdict.txt\"\n",
    "core.download_textfile_from_url(url, file_path)\n",
    "\n",
    "# check the data\n",
    "!ls -1 ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a938894-56ad-415c-b210-e88da08b6a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "==================================================\n",
      "Printing the first 100 characters: \n",
      " I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "# See the content of the file\n",
    "raw_text = core.read_file(file_path)\n",
    "\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print('=' * 50)\n",
    "\n",
    "N = 100\n",
    "print(f'Printing the first {N} characters: \\n', raw_text[:N])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45909a98-2408-4e8b-8008-18633873f457",
   "metadata": {},
   "source": [
    "Before using any pre-built tokenizer, let's have some fun and use `re`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f3d90f9-2c9b-4103-a8cb-0e4e30d03710",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Let's split at white space, comma, and period\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# As example, let's show it for the first N characters\u001b[39;00m\n\u001b[1;32m      3\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mre\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[.,]|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m, raw_text[:N])\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's split at white space, comma, and period\n",
    "# As example, let's show it for the first N characters\n",
    "N = 150\n",
    "re.split(r'[.,]|\\s', raw_text[:N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17eca1d1-4a88-4c78-b537-6540e0a38c9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# let's get rid of empty strings\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m raw_text_split \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[.,]|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m, raw_text)\n\u001b[1;32m      3\u001b[0m raw_text_split \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m raw_text_split \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "# let's get rid of empty strings\n",
    "raw_text_split = re.split(r'[.,]|\\s', raw_text)\n",
    "raw_text_split = [x for x in raw_text_split if x != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b7e0cd2-66ea-446c-9c22-96d8e59d6c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing top 10 tokens: \n",
      " ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius']\n"
     ]
    }
   ],
   "source": [
    "# Now let's try a more-complicated regex that can cover other special characters\n",
    "# as well as double dashes\n",
    "\n",
    "regex_pattern = r'([,.:;?_!\"()\\']|--|\\s)'\n",
    "tokens = core.tokenizer(raw_text, regex_pattern, remove_white_spaces=True)\n",
    "\n",
    "print(f\"showing top 10 tokens: \\n\", tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6098ee-afeb-47c4-ac1d-ca8ac20d6f79",
   "metadata": {},
   "source": [
    "One important note mentioned by the author is that we prefer to not change the capital letters to lower cases or vice versa. This is because the goal is to prep data that helps model with its task, i.e. predicting the next token. Therefore, the capital letters are kept as they may help model to better understand the start of sentence, the name of people, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e8691-b971-4ada-9adf-5dd1fb926ae6",
   "metadata": {},
   "source": [
    "### Step 2. Convert/Map `tokens` into `token IDs`\n",
    "\n",
    "This is an intermediary step that needs to occur before obtaining the embedding vectors\n",
    "\n",
    "**Question:** Why do we need this step?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c026d822-b26a-46c9-8145-83447b47a0f6",
   "metadata": {},
   "source": [
    "Basically, we want to dedicate a unique ID to each unique token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18ea539b-7cdd-4da4-ac82-43d3a2da43ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '\"': 1,\n",
       " \"'\": 2,\n",
       " '(': 3,\n",
       " ')': 4,\n",
       " ',': 5,\n",
       " '--': 6,\n",
       " '.': 7,\n",
       " ':': 8,\n",
       " ';': 9,\n",
       " '?': 10,\n",
       " 'A': 11,\n",
       " 'Ah': 12,\n",
       " 'Among': 13,\n",
       " 'And': 14,\n",
       " 'Are': 15,\n",
       " 'Arrt': 16,\n",
       " 'As': 17,\n",
       " 'At': 18,\n",
       " 'Be': 19,\n",
       " 'Begin': 20,\n",
       " 'Burlington': 21,\n",
       " 'But': 22,\n",
       " 'By': 23,\n",
       " 'Carlo': 24,\n",
       " 'Chicago': 25,\n",
       " 'Claude': 26,\n",
       " 'Come': 27,\n",
       " 'Croft': 28,\n",
       " 'Destroyed': 29,\n",
       " 'Devonshire': 30,\n",
       " 'Don': 31,\n",
       " 'Dubarry': 32,\n",
       " 'Emperors': 33,\n",
       " 'Florence': 34,\n",
       " 'For': 35,\n",
       " 'Gallery': 36,\n",
       " 'Gideon': 37,\n",
       " 'Gisburn': 38,\n",
       " 'Gisburns': 39,\n",
       " 'Grafton': 40,\n",
       " 'Greek': 41,\n",
       " 'Grindle': 42,\n",
       " 'Grindles': 43,\n",
       " 'HAD': 44,\n",
       " 'Had': 45,\n",
       " 'Hang': 46,\n",
       " 'Has': 47,\n",
       " 'He': 48,\n",
       " 'Her': 49,\n",
       " 'Hermia': 50,\n",
       " 'His': 51,\n",
       " 'How': 52,\n",
       " 'I': 53,\n",
       " 'If': 54,\n",
       " 'In': 55,\n",
       " 'It': 56,\n",
       " 'Jack': 57,\n",
       " 'Jove': 58,\n",
       " 'Just': 59,\n",
       " 'Lord': 60,\n",
       " 'Made': 61,\n",
       " 'Miss': 62,\n",
       " 'Money': 63,\n",
       " 'Monte': 64,\n",
       " 'Moon-dancers': 65,\n",
       " 'Mr': 66,\n",
       " 'Mrs': 67,\n",
       " 'My': 68,\n",
       " 'Never': 69,\n",
       " 'No': 70,\n",
       " 'Now': 71,\n",
       " 'Nutley': 72,\n",
       " 'Of': 73,\n",
       " 'Oh': 74,\n",
       " 'On': 75,\n",
       " 'Once': 76,\n",
       " 'Only': 77,\n",
       " 'Or': 78,\n",
       " 'Perhaps': 79,\n",
       " 'Poor': 80,\n",
       " 'Professional': 81,\n",
       " 'Renaissance': 82,\n",
       " 'Rickham': 83,\n",
       " 'Riviera': 84,\n",
       " 'Rome': 85,\n",
       " 'Russian': 86,\n",
       " 'Sevres': 87,\n",
       " 'She': 88,\n",
       " 'Stroud': 89,\n",
       " 'Strouds': 90,\n",
       " 'Suddenly': 91,\n",
       " 'That': 92,\n",
       " 'The': 93,\n",
       " 'Then': 94,\n",
       " 'There': 95,\n",
       " 'They': 96,\n",
       " 'This': 97,\n",
       " 'Those': 98,\n",
       " 'Though': 99,\n",
       " 'Thwing': 100,\n",
       " 'Thwings': 101,\n",
       " 'To': 102,\n",
       " 'Usually': 103,\n",
       " 'Venetian': 104,\n",
       " 'Victor': 105,\n",
       " 'Was': 106,\n",
       " 'We': 107,\n",
       " 'Well': 108,\n",
       " 'What': 109,\n",
       " 'When': 110,\n",
       " 'Why': 111,\n",
       " 'Yes': 112,\n",
       " 'You': 113,\n",
       " '_': 114,\n",
       " 'a': 115,\n",
       " 'abdication': 116,\n",
       " 'able': 117,\n",
       " 'about': 118,\n",
       " 'above': 119,\n",
       " 'abruptly': 120,\n",
       " 'absolute': 121,\n",
       " 'absorbed': 122,\n",
       " 'absurdity': 123,\n",
       " 'academic': 124,\n",
       " 'accuse': 125,\n",
       " 'accustomed': 126,\n",
       " 'across': 127,\n",
       " 'activity': 128,\n",
       " 'add': 129,\n",
       " 'added': 130,\n",
       " 'admirers': 131,\n",
       " 'adopted': 132,\n",
       " 'adulation': 133,\n",
       " 'advance': 134,\n",
       " 'aesthetic': 135,\n",
       " 'affect': 136,\n",
       " 'afraid': 137,\n",
       " 'after': 138,\n",
       " 'afterward': 139,\n",
       " 'again': 140,\n",
       " 'ago': 141,\n",
       " 'ah': 142,\n",
       " 'air': 143,\n",
       " 'alive': 144,\n",
       " 'all': 145,\n",
       " 'almost': 146,\n",
       " 'alone': 147,\n",
       " 'along': 148,\n",
       " 'always': 149,\n",
       " 'am': 150,\n",
       " 'amazement': 151,\n",
       " 'amid': 152,\n",
       " 'among': 153,\n",
       " 'amplest': 154,\n",
       " 'amusing': 155,\n",
       " 'an': 156,\n",
       " 'and': 157,\n",
       " 'another': 158,\n",
       " 'answer': 159,\n",
       " 'answered': 160,\n",
       " 'any': 161,\n",
       " 'anything': 162,\n",
       " 'anywhere': 163,\n",
       " 'apparent': 164,\n",
       " 'apparently': 165,\n",
       " 'appearance': 166,\n",
       " 'appeared': 167,\n",
       " 'appointed': 168,\n",
       " 'are': 169,\n",
       " 'arm': 170,\n",
       " 'arm-chair': 171,\n",
       " 'arm-chairs': 172,\n",
       " 'arms': 173,\n",
       " 'art': 174,\n",
       " 'articles': 175,\n",
       " 'artist': 176,\n",
       " 'as': 177,\n",
       " 'aside': 178,\n",
       " 'asked': 179,\n",
       " 'at': 180,\n",
       " 'atmosphere': 181,\n",
       " 'atom': 182,\n",
       " 'attack': 183,\n",
       " 'attention': 184,\n",
       " 'attitude': 185,\n",
       " 'audacities': 186,\n",
       " 'away': 187,\n",
       " 'awful': 188,\n",
       " 'axioms': 189,\n",
       " 'azaleas': 190,\n",
       " 'back': 191,\n",
       " 'background': 192,\n",
       " 'balance': 193,\n",
       " 'balancing': 194,\n",
       " 'balustraded': 195,\n",
       " 'basking': 196,\n",
       " 'bath-rooms': 197,\n",
       " 'be': 198,\n",
       " 'beaming': 199,\n",
       " 'bean-stalk': 200,\n",
       " 'bear': 201,\n",
       " 'beard': 202,\n",
       " 'beauty': 203,\n",
       " 'became': 204,\n",
       " 'because': 205,\n",
       " 'becoming': 206,\n",
       " 'bed': 207,\n",
       " 'been': 208,\n",
       " 'before': 209,\n",
       " 'began': 210,\n",
       " 'begun': 211,\n",
       " 'behind': 212,\n",
       " 'being': 213,\n",
       " 'believed': 214,\n",
       " 'beneath': 215,\n",
       " 'bespoke': 216,\n",
       " 'better': 217,\n",
       " 'between': 218,\n",
       " 'big': 219,\n",
       " 'bits': 220,\n",
       " 'bitterness': 221,\n",
       " 'blocked': 222,\n",
       " 'born': 223,\n",
       " 'borne': 224,\n",
       " 'boudoir': 225,\n",
       " 'bravura': 226,\n",
       " 'break': 227,\n",
       " 'breaking': 228,\n",
       " 'breathing': 229,\n",
       " 'bric-a-brac': 230,\n",
       " 'briefly': 231,\n",
       " 'brings': 232,\n",
       " 'bronzes': 233,\n",
       " 'brought': 234,\n",
       " 'brown': 235,\n",
       " 'brush': 236,\n",
       " 'bull': 237,\n",
       " 'business': 238,\n",
       " 'but': 239,\n",
       " 'buying': 240,\n",
       " 'by': 241,\n",
       " 'called': 242,\n",
       " 'came': 243,\n",
       " 'can': 244,\n",
       " 'canvas': 245,\n",
       " 'canvases': 246,\n",
       " 'cards': 247,\n",
       " 'care': 248,\n",
       " 'career': 249,\n",
       " 'caught': 250,\n",
       " 'central': 251,\n",
       " 'chair': 252,\n",
       " 'chap': 253,\n",
       " 'characteristic': 254,\n",
       " 'charming': 255,\n",
       " 'cheap': 256,\n",
       " 'check': 257,\n",
       " 'cheeks': 258,\n",
       " 'chest': 259,\n",
       " 'chimney-piece': 260,\n",
       " 'chucked': 261,\n",
       " 'cigar': 262,\n",
       " 'cigarette': 263,\n",
       " 'cigars': 264,\n",
       " 'circulation': 265,\n",
       " 'circumstance': 266,\n",
       " 'circus-clown': 267,\n",
       " 'claimed': 268,\n",
       " 'clasping': 269,\n",
       " 'clear': 270,\n",
       " 'cleverer': 271,\n",
       " 'close': 272,\n",
       " 'clue': 273,\n",
       " 'coat': 274,\n",
       " 'collapsed': 275,\n",
       " 'colour': 276,\n",
       " 'come': 277,\n",
       " 'comfortable': 278,\n",
       " 'coming': 279,\n",
       " 'companion': 280,\n",
       " 'compared': 281,\n",
       " 'complex': 282,\n",
       " 'confident': 283,\n",
       " 'congesting': 284,\n",
       " 'conjugal': 285,\n",
       " 'constraint': 286,\n",
       " 'consummate': 287,\n",
       " 'contended': 288,\n",
       " 'continued': 289,\n",
       " 'corner': 290,\n",
       " 'corrected': 291,\n",
       " 'could': 292,\n",
       " 'couldn': 293,\n",
       " 'count': 294,\n",
       " 'countenance': 295,\n",
       " 'couple': 296,\n",
       " 'course': 297,\n",
       " 'covered': 298,\n",
       " 'craft': 299,\n",
       " 'cried': 300,\n",
       " 'crossed': 301,\n",
       " 'crowned': 302,\n",
       " 'crumbled': 303,\n",
       " 'cry': 304,\n",
       " 'cured': 305,\n",
       " 'curiosity': 306,\n",
       " 'curious': 307,\n",
       " 'current': 308,\n",
       " 'curtains': 309,\n",
       " 'd': 310,\n",
       " 'dabble': 311,\n",
       " 'damask': 312,\n",
       " 'dark': 313,\n",
       " 'dashed': 314,\n",
       " 'day': 315,\n",
       " 'days': 316,\n",
       " 'dead': 317,\n",
       " 'deadening': 318,\n",
       " 'dear': 319,\n",
       " 'deep': 320,\n",
       " 'deerhound': 321,\n",
       " 'degree': 322,\n",
       " 'delicate': 323,\n",
       " 'demand': 324,\n",
       " 'denied': 325,\n",
       " 'deploring': 326,\n",
       " 'deprecating': 327,\n",
       " 'deprecatingly': 328,\n",
       " 'desire': 329,\n",
       " 'destroyed': 330,\n",
       " 'destruction': 331,\n",
       " 'desultory': 332,\n",
       " 'detail': 333,\n",
       " 'diagnosis': 334,\n",
       " 'did': 335,\n",
       " 'didn': 336,\n",
       " 'died': 337,\n",
       " 'dim': 338,\n",
       " 'dimmest': 339,\n",
       " 'dingy': 340,\n",
       " 'dining-room': 341,\n",
       " 'disarming': 342,\n",
       " 'discovery': 343,\n",
       " 'discrimination': 344,\n",
       " 'discussion': 345,\n",
       " 'disdain': 346,\n",
       " 'disdained': 347,\n",
       " 'disease': 348,\n",
       " 'disguised': 349,\n",
       " 'display': 350,\n",
       " 'dissatisfied': 351,\n",
       " 'distinguished': 352,\n",
       " 'distract': 353,\n",
       " 'divert': 354,\n",
       " 'do': 355,\n",
       " 'doesn': 356,\n",
       " 'doing': 357,\n",
       " 'domestic': 358,\n",
       " 'don': 359,\n",
       " 'done': 360,\n",
       " 'donkey': 361,\n",
       " 'down': 362,\n",
       " 'dozen': 363,\n",
       " 'dragged': 364,\n",
       " 'drawing-room': 365,\n",
       " 'drawing-rooms': 366,\n",
       " 'drawn': 367,\n",
       " 'dress-closets': 368,\n",
       " 'drew': 369,\n",
       " 'dropped': 370,\n",
       " 'each': 371,\n",
       " 'earth': 372,\n",
       " 'ease': 373,\n",
       " 'easel': 374,\n",
       " 'easy': 375,\n",
       " 'echoed': 376,\n",
       " 'economy': 377,\n",
       " 'effect': 378,\n",
       " 'effects': 379,\n",
       " 'efforts': 380,\n",
       " 'egregious': 381,\n",
       " 'eighteenth-century': 382,\n",
       " 'elbow': 383,\n",
       " 'elegant': 384,\n",
       " 'else': 385,\n",
       " 'embarrassed': 386,\n",
       " 'enabled': 387,\n",
       " 'end': 388,\n",
       " 'endless': 389,\n",
       " 'enjoy': 390,\n",
       " 'enlightenment': 391,\n",
       " 'enough': 392,\n",
       " 'ensuing': 393,\n",
       " 'equally': 394,\n",
       " 'equanimity': 395,\n",
       " 'escape': 396,\n",
       " 'established': 397,\n",
       " 'etching': 398,\n",
       " 'even': 399,\n",
       " 'event': 400,\n",
       " 'ever': 401,\n",
       " 'everlasting': 402,\n",
       " 'every': 403,\n",
       " 'exasperated': 404,\n",
       " 'except': 405,\n",
       " 'excuse': 406,\n",
       " 'excusing': 407,\n",
       " 'existed': 408,\n",
       " 'expected': 409,\n",
       " 'exquisite': 410,\n",
       " 'exquisitely': 411,\n",
       " 'extenuation': 412,\n",
       " 'exterminating': 413,\n",
       " 'extracting': 414,\n",
       " 'eye': 415,\n",
       " 'eyebrows': 416,\n",
       " 'eyes': 417,\n",
       " 'face': 418,\n",
       " 'faces': 419,\n",
       " 'fact': 420,\n",
       " 'faded': 421,\n",
       " 'failed': 422,\n",
       " 'failure': 423,\n",
       " 'fair': 424,\n",
       " 'faith': 425,\n",
       " 'false': 426,\n",
       " 'familiar': 427,\n",
       " 'famille-verte': 428,\n",
       " 'fancy': 429,\n",
       " 'fashionable': 430,\n",
       " 'fate': 431,\n",
       " 'feather': 432,\n",
       " 'feet': 433,\n",
       " 'fell': 434,\n",
       " 'fellow': 435,\n",
       " 'felt': 436,\n",
       " 'few': 437,\n",
       " 'fewer': 438,\n",
       " 'finality': 439,\n",
       " 'find': 440,\n",
       " 'fingers': 441,\n",
       " 'first': 442,\n",
       " 'fit': 443,\n",
       " 'fitting': 444,\n",
       " 'five': 445,\n",
       " 'flash': 446,\n",
       " 'flashed': 447,\n",
       " 'florid': 448,\n",
       " 'flowers': 449,\n",
       " 'fluently': 450,\n",
       " 'flung': 451,\n",
       " 'follow': 452,\n",
       " 'followed': 453,\n",
       " 'fond': 454,\n",
       " 'footstep': 455,\n",
       " 'for': 456,\n",
       " 'forced': 457,\n",
       " 'forcing': 458,\n",
       " 'forehead': 459,\n",
       " 'foreign': 460,\n",
       " 'foreseen': 461,\n",
       " 'forgive': 462,\n",
       " 'forgotten': 463,\n",
       " 'form': 464,\n",
       " 'formed': 465,\n",
       " 'forming': 466,\n",
       " 'forward': 467,\n",
       " 'fostered': 468,\n",
       " 'found': 469,\n",
       " 'foundations': 470,\n",
       " 'fragment': 471,\n",
       " 'fragments': 472,\n",
       " 'frame': 473,\n",
       " 'frames': 474,\n",
       " 'frequently': 475,\n",
       " 'friend': 476,\n",
       " 'from': 477,\n",
       " 'full': 478,\n",
       " 'fullest': 479,\n",
       " 'furiously': 480,\n",
       " 'furrowed': 481,\n",
       " 'garlanded': 482,\n",
       " 'garlands': 483,\n",
       " 'gave': 484,\n",
       " 'genial': 485,\n",
       " 'genius': 486,\n",
       " 'gesture': 487,\n",
       " 'get': 488,\n",
       " 'getting': 489,\n",
       " 'give': 490,\n",
       " 'given': 491,\n",
       " 'glad': 492,\n",
       " 'glanced': 493,\n",
       " 'glimpse': 494,\n",
       " 'gloried': 495,\n",
       " 'glory': 496,\n",
       " 'go': 497,\n",
       " 'going': 498,\n",
       " 'gone': 499,\n",
       " 'good': 500,\n",
       " 'good-breeding': 501,\n",
       " 'good-humoured': 502,\n",
       " 'got': 503,\n",
       " 'grace': 504,\n",
       " 'gradually': 505,\n",
       " 'gray': 506,\n",
       " 'grayish': 507,\n",
       " 'great': 508,\n",
       " 'greatest': 509,\n",
       " 'greatness': 510,\n",
       " 'grew': 511,\n",
       " 'groping': 512,\n",
       " 'growing': 513,\n",
       " 'had': 514,\n",
       " 'hadn': 515,\n",
       " 'hair': 516,\n",
       " 'half': 517,\n",
       " 'half-light': 518,\n",
       " 'half-mechanically': 519,\n",
       " 'hall': 520,\n",
       " 'hand': 521,\n",
       " 'hands': 522,\n",
       " 'handsome': 523,\n",
       " 'hanging': 524,\n",
       " 'happen': 525,\n",
       " 'happened': 526,\n",
       " 'hard': 527,\n",
       " 'hardly': 528,\n",
       " 'has': 529,\n",
       " 'have': 530,\n",
       " 'haven': 531,\n",
       " 'having': 532,\n",
       " 'he': 533,\n",
       " 'head': 534,\n",
       " 'hear': 535,\n",
       " 'heard': 536,\n",
       " 'heart': 537,\n",
       " 'height': 538,\n",
       " 'her': 539,\n",
       " 'here': 540,\n",
       " 'hermit': 541,\n",
       " 'herself': 542,\n",
       " 'hesitations': 543,\n",
       " 'hide': 544,\n",
       " 'high': 545,\n",
       " 'him': 546,\n",
       " 'himself': 547,\n",
       " 'hint': 548,\n",
       " 'his': 549,\n",
       " 'history': 550,\n",
       " 'holding': 551,\n",
       " 'home': 552,\n",
       " 'honour': 553,\n",
       " 'hooded': 554,\n",
       " 'hostess': 555,\n",
       " 'hot-house': 556,\n",
       " 'hour': 557,\n",
       " 'hours': 558,\n",
       " 'house': 559,\n",
       " 'how': 560,\n",
       " 'hung': 561,\n",
       " 'husband': 562,\n",
       " 'idea': 563,\n",
       " 'idle': 564,\n",
       " 'idling': 565,\n",
       " 'if': 566,\n",
       " 'immediately': 567,\n",
       " 'in': 568,\n",
       " 'incense': 569,\n",
       " 'indifferent': 570,\n",
       " 'inevitable': 571,\n",
       " 'inevitably': 572,\n",
       " 'inflexible': 573,\n",
       " 'insensible': 574,\n",
       " 'insignificant': 575,\n",
       " 'instinctively': 576,\n",
       " 'instructive': 577,\n",
       " 'interesting': 578,\n",
       " 'into': 579,\n",
       " 'ironic': 580,\n",
       " 'irony': 581,\n",
       " 'irrelevance': 582,\n",
       " 'irrevocable': 583,\n",
       " 'is': 584,\n",
       " 'it': 585,\n",
       " 'its': 586,\n",
       " 'itself': 587,\n",
       " 'jardiniere': 588,\n",
       " 'jealousy': 589,\n",
       " 'just': 590,\n",
       " 'keep': 591,\n",
       " 'kept': 592,\n",
       " 'kind': 593,\n",
       " 'knees': 594,\n",
       " 'knew': 595,\n",
       " 'know': 596,\n",
       " 'known': 597,\n",
       " 'laid': 598,\n",
       " 'lair': 599,\n",
       " 'landing': 600,\n",
       " 'language': 601,\n",
       " 'last': 602,\n",
       " 'late': 603,\n",
       " 'later': 604,\n",
       " 'latter': 605,\n",
       " 'laugh': 606,\n",
       " 'laughed': 607,\n",
       " 'lay': 608,\n",
       " 'leading': 609,\n",
       " 'lean': 610,\n",
       " 'learned': 611,\n",
       " 'least': 612,\n",
       " 'leathery': 613,\n",
       " 'leave': 614,\n",
       " 'led': 615,\n",
       " 'left': 616,\n",
       " 'leisure': 617,\n",
       " 'lends': 618,\n",
       " 'lent': 619,\n",
       " 'let': 620,\n",
       " 'lies': 621,\n",
       " 'life': 622,\n",
       " 'life-likeness': 623,\n",
       " 'lift': 624,\n",
       " 'lifted': 625,\n",
       " 'light': 626,\n",
       " 'lightly': 627,\n",
       " 'like': 628,\n",
       " 'liked': 629,\n",
       " 'line': 630,\n",
       " 'lines': 631,\n",
       " 'lingered': 632,\n",
       " 'lips': 633,\n",
       " 'lit': 634,\n",
       " 'little': 635,\n",
       " 'live': 636,\n",
       " 'll': 637,\n",
       " 'loathing': 638,\n",
       " 'long': 639,\n",
       " 'longed': 640,\n",
       " 'longer': 641,\n",
       " 'look': 642,\n",
       " 'looked': 643,\n",
       " 'looking': 644,\n",
       " 'lose': 645,\n",
       " 'loss': 646,\n",
       " 'lounging': 647,\n",
       " 'lovely': 648,\n",
       " 'lucky': 649,\n",
       " 'lump': 650,\n",
       " 'luncheon-table': 651,\n",
       " 'luxury': 652,\n",
       " 'lying': 653,\n",
       " 'made': 654,\n",
       " 'make': 655,\n",
       " 'man': 656,\n",
       " 'manage': 657,\n",
       " 'managed': 658,\n",
       " 'mantel-piece': 659,\n",
       " 'marble': 660,\n",
       " 'married': 661,\n",
       " 'may': 662,\n",
       " 'me': 663,\n",
       " 'meant': 664,\n",
       " 'mediocrity': 665,\n",
       " 'medium': 666,\n",
       " 'mentioned': 667,\n",
       " 'mere': 668,\n",
       " 'merely': 669,\n",
       " 'met': 670,\n",
       " 'might': 671,\n",
       " 'mighty': 672,\n",
       " 'millionaire': 673,\n",
       " 'mine': 674,\n",
       " 'minute': 675,\n",
       " 'minutes': 676,\n",
       " 'mirrors': 677,\n",
       " 'modest': 678,\n",
       " 'modesty': 679,\n",
       " 'moment': 680,\n",
       " 'money': 681,\n",
       " 'monumental': 682,\n",
       " 'mood': 683,\n",
       " 'morbidly': 684,\n",
       " 'more': 685,\n",
       " 'most': 686,\n",
       " 'mourn': 687,\n",
       " 'mourned': 688,\n",
       " 'moustache': 689,\n",
       " 'moved': 690,\n",
       " 'much': 691,\n",
       " 'muddling': 692,\n",
       " 'multiplied': 693,\n",
       " 'murmur': 694,\n",
       " 'muscles': 695,\n",
       " 'must': 696,\n",
       " 'my': 697,\n",
       " 'myself': 698,\n",
       " 'mysterious': 699,\n",
       " 'naive': 700,\n",
       " 'near': 701,\n",
       " 'nearly': 702,\n",
       " 'negatived': 703,\n",
       " 'nervous': 704,\n",
       " 'nervousness': 705,\n",
       " 'neutral': 706,\n",
       " 'never': 707,\n",
       " 'next': 708,\n",
       " 'no': 709,\n",
       " 'none': 710,\n",
       " 'not': 711,\n",
       " 'note': 712,\n",
       " 'nothing': 713,\n",
       " 'now': 714,\n",
       " 'nymphs': 715,\n",
       " 'oak': 716,\n",
       " 'obituary': 717,\n",
       " 'object': 718,\n",
       " 'objects': 719,\n",
       " 'occurred': 720,\n",
       " 'oddly': 721,\n",
       " 'of': 722,\n",
       " 'off': 723,\n",
       " 'often': 724,\n",
       " 'oh': 725,\n",
       " 'old': 726,\n",
       " 'on': 727,\n",
       " 'once': 728,\n",
       " 'one': 729,\n",
       " 'ones': 730,\n",
       " 'only': 731,\n",
       " 'onto': 732,\n",
       " 'open': 733,\n",
       " 'or': 734,\n",
       " 'other': 735,\n",
       " 'our': 736,\n",
       " 'ourselves': 737,\n",
       " 'out': 738,\n",
       " 'outline': 739,\n",
       " 'oval': 740,\n",
       " 'over': 741,\n",
       " 'own': 742,\n",
       " 'packed': 743,\n",
       " 'paid': 744,\n",
       " 'paint': 745,\n",
       " 'painted': 746,\n",
       " 'painter': 747,\n",
       " 'painting': 748,\n",
       " 'pale': 749,\n",
       " 'paled': 750,\n",
       " 'palm-trees': 751,\n",
       " 'panel': 752,\n",
       " 'panelling': 753,\n",
       " 'pardonable': 754,\n",
       " 'pardoned': 755,\n",
       " 'part': 756,\n",
       " 'passages': 757,\n",
       " 'passing': 758,\n",
       " 'past': 759,\n",
       " 'pastels': 760,\n",
       " 'pathos': 761,\n",
       " 'patient': 762,\n",
       " 'people': 763,\n",
       " 'perceptible': 764,\n",
       " 'perfect': 765,\n",
       " 'persistence': 766,\n",
       " 'persuasively': 767,\n",
       " 'phrase': 768,\n",
       " 'picture': 769,\n",
       " 'pictures': 770,\n",
       " 'pines': 771,\n",
       " 'pink': 772,\n",
       " 'place': 773,\n",
       " 'placed': 774,\n",
       " 'plain': 775,\n",
       " 'platitudes': 776,\n",
       " 'pleased': 777,\n",
       " 'pockets': 778,\n",
       " 'point': 779,\n",
       " 'poised': 780,\n",
       " 'poor': 781,\n",
       " 'portrait': 782,\n",
       " 'posing': 783,\n",
       " 'possessed': 784,\n",
       " 'poverty': 785,\n",
       " 'predicted': 786,\n",
       " 'preliminary': 787,\n",
       " 'presenting': 788,\n",
       " 'prestidigitation': 789,\n",
       " 'pretty': 790,\n",
       " 'previous': 791,\n",
       " 'price': 792,\n",
       " 'pride': 793,\n",
       " 'princely': 794,\n",
       " 'prism': 795,\n",
       " 'problem': 796,\n",
       " 'proclaiming': 797,\n",
       " 'prodigious': 798,\n",
       " 'profusion': 799,\n",
       " 'protest': 800,\n",
       " 'prove': 801,\n",
       " 'public': 802,\n",
       " 'purblind': 803,\n",
       " 'purely': 804,\n",
       " 'pushed': 805,\n",
       " 'put': 806,\n",
       " 'qualities': 807,\n",
       " 'quality': 808,\n",
       " 'queerly': 809,\n",
       " 'question': 810,\n",
       " 'quickly': 811,\n",
       " 'quietly': 812,\n",
       " 'quite': 813,\n",
       " 'quote': 814,\n",
       " 'rain': 815,\n",
       " 'raised': 816,\n",
       " 'random': 817,\n",
       " 'rather': 818,\n",
       " 're': 819,\n",
       " 'real': 820,\n",
       " 'really': 821,\n",
       " 'reared': 822,\n",
       " 'reason': 823,\n",
       " 'reassurance': 824,\n",
       " 'recovering': 825,\n",
       " 'recreated': 826,\n",
       " 'reflected': 827,\n",
       " 'reflection': 828,\n",
       " 'regrets': 829,\n",
       " 'relatively': 830,\n",
       " 'remained': 831,\n",
       " 'remember': 832,\n",
       " 'reminded': 833,\n",
       " 'repeating': 834,\n",
       " 'represented': 835,\n",
       " 'reproduction': 836,\n",
       " 'resented': 837,\n",
       " 'resolve': 838,\n",
       " 'resources': 839,\n",
       " 'rest': 840,\n",
       " 'rich': 841,\n",
       " 'ridiculous': 842,\n",
       " 'robbed': 843,\n",
       " 'romantic': 844,\n",
       " 'room': 845,\n",
       " 'rose': 846,\n",
       " 'rs': 847,\n",
       " 'rule': 848,\n",
       " 'run': 849,\n",
       " 's': 850,\n",
       " 'said': 851,\n",
       " 'same': 852,\n",
       " 'satisfaction': 853,\n",
       " 'savour': 854,\n",
       " 'saw': 855,\n",
       " 'say': 856,\n",
       " 'saying': 857,\n",
       " 'says': 858,\n",
       " 'scorn': 859,\n",
       " 'scornful': 860,\n",
       " 'secret': 861,\n",
       " 'see': 862,\n",
       " 'seemed': 863,\n",
       " 'seen': 864,\n",
       " 'self-confident': 865,\n",
       " 'send': 866,\n",
       " 'sensation': 867,\n",
       " 'sensitive': 868,\n",
       " 'sent': 869,\n",
       " 'serious': 870,\n",
       " 'set': 871,\n",
       " 'sex': 872,\n",
       " 'shade': 873,\n",
       " 'shaking': 874,\n",
       " 'shall': 875,\n",
       " 'she': 876,\n",
       " 'shirked': 877,\n",
       " 'short': 878,\n",
       " 'should': 879,\n",
       " 'shoulder': 880,\n",
       " 'shoulders': 881,\n",
       " 'show': 882,\n",
       " 'showed': 883,\n",
       " 'showy': 884,\n",
       " 'shrug': 885,\n",
       " 'shrugged': 886,\n",
       " 'sight': 887,\n",
       " 'sign': 888,\n",
       " 'silent': 889,\n",
       " 'silver': 890,\n",
       " 'similar': 891,\n",
       " 'simpleton': 892,\n",
       " 'simplifications': 893,\n",
       " 'simply': 894,\n",
       " 'since': 895,\n",
       " 'single': 896,\n",
       " 'sitter': 897,\n",
       " 'sitters': 898,\n",
       " 'sketch': 899,\n",
       " 'skill': 900,\n",
       " 'slight': 901,\n",
       " 'slightly': 902,\n",
       " 'slowly': 903,\n",
       " 'small': 904,\n",
       " 'smile': 905,\n",
       " 'smiling': 906,\n",
       " 'sneer': 907,\n",
       " 'so': 908,\n",
       " 'solace': 909,\n",
       " 'some': 910,\n",
       " 'somebody': 911,\n",
       " 'something': 912,\n",
       " 'spacious': 913,\n",
       " 'spaniel': 914,\n",
       " 'speaking-tubes': 915,\n",
       " 'speculations': 916,\n",
       " 'spite': 917,\n",
       " 'splash': 918,\n",
       " 'square': 919,\n",
       " 'stairs': 920,\n",
       " 'stammer': 921,\n",
       " 'stand': 922,\n",
       " 'standing': 923,\n",
       " 'started': 924,\n",
       " 'stay': 925,\n",
       " 'still': 926,\n",
       " 'stocked': 927,\n",
       " 'stood': 928,\n",
       " 'stopped': 929,\n",
       " 'stopping': 930,\n",
       " 'straddling': 931,\n",
       " 'straight': 932,\n",
       " 'strain': 933,\n",
       " 'straining': 934,\n",
       " 'strange': 935,\n",
       " 'straw': 936,\n",
       " 'stream': 937,\n",
       " 'stroke': 938,\n",
       " 'strokes': 939,\n",
       " 'strolled': 940,\n",
       " 'strongest': 941,\n",
       " 'strongly': 942,\n",
       " 'struck': 943,\n",
       " 'studio': 944,\n",
       " 'stuff': 945,\n",
       " 'subject': 946,\n",
       " 'substantial': 947,\n",
       " 'suburban': 948,\n",
       " 'such': 949,\n",
       " 'suddenly': 950,\n",
       " 'suffered': 951,\n",
       " 'sugar': 952,\n",
       " 'suggested': 953,\n",
       " 'sunburn': 954,\n",
       " 'sunburnt': 955,\n",
       " 'sunlit': 956,\n",
       " 'superb': 957,\n",
       " 'sure': 958,\n",
       " 'surest': 959,\n",
       " 'surface': 960,\n",
       " 'surprise': 961,\n",
       " 'surprised': 962,\n",
       " 'surrounded': 963,\n",
       " 'suspected': 964,\n",
       " 'sweetly': 965,\n",
       " 'sweetness': 966,\n",
       " 'swelling': 967,\n",
       " 'swept': 968,\n",
       " 'swum': 969,\n",
       " 't': 970,\n",
       " 'table': 971,\n",
       " 'take': 972,\n",
       " 'taken': 973,\n",
       " 'talking': 974,\n",
       " 'tea': 975,\n",
       " 'tears': 976,\n",
       " 'technicalities': 977,\n",
       " 'technique': 978,\n",
       " 'tell': 979,\n",
       " 'tells': 980,\n",
       " 'tempting': 981,\n",
       " 'terra-cotta': 982,\n",
       " 'terrace': 983,\n",
       " 'terraces': 984,\n",
       " 'terribly': 985,\n",
       " 'than': 986,\n",
       " 'that': 987,\n",
       " 'the': 988,\n",
       " 'their': 989,\n",
       " 'them': 990,\n",
       " 'then': 991,\n",
       " 'there': 992,\n",
       " 'therefore': 993,\n",
       " 'they': 994,\n",
       " 'thin': 995,\n",
       " 'thing': 996,\n",
       " 'things': 997,\n",
       " 'think': 998,\n",
       " 'this': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tokens = sorted(set(tokens))\n",
    "vocabulary = {\n",
    "    token: token_id \n",
    "    for token_id, token in enumerate(unique_tokens)\n",
    "}\n",
    "\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cdd15d-44e3-4bc5-bffb-c11eae92cebb",
   "metadata": {},
   "source": [
    "Now that we have our `vocabulary`, we can use it to convert a sample text to its corresponding token ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "899eb2ec-c654-4c15-aaae-df0650fed95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[999, 584, None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:\n",
    "sample_txt = \"this is awesome\"\n",
    "sample_tokens = core.tokenizer(sample_txt, regex_pattern)\n",
    "sample_token_ids = [vocabulary.get(t, None) for t in sample_tokens]\n",
    "\n",
    "sample_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73859f91-0f70-4e3e-934e-baeb0b2ef579",
   "metadata": {},
   "source": [
    "Note that it is possible to encounter a word that is completely new and hence has no `token_id` in our vocabulary. To this end, we may use `-1`, or keep updating our vocabulary (??). We may read more about this in the book. If not, let's google it later!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815a4b43-79dd-4692-a5b9-689d770cb218",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "The author used OOP to create a class for tokenizer. Why not a function? IIUC, this is because there are two atributes that are tied to each other, and we need to do two operations where one is the reverse of the other. So, it is about tracking things, and, in this case, `class` seems to be a good option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99aea2f4-98e2-417f-8327-be7655d547c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "text = \"\"\"It's the last he painted, you know,\"\n",
    "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "token_transformer = core.TokenTransform(vocabulary, regex_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d664136d-66d0-40fc-97f7-ddc1687489a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56,\n",
       " 2,\n",
       " 850,\n",
       " 988,\n",
       " 602,\n",
       " 533,\n",
       " 746,\n",
       " 5,\n",
       " 1126,\n",
       " 596,\n",
       " 5,\n",
       " 1,\n",
       " 67,\n",
       " 7,\n",
       " 38,\n",
       " 851,\n",
       " 1108,\n",
       " 754,\n",
       " 793,\n",
       " 7]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_id = token_transformer.encode(text)\n",
    "text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42a28c02-b96e-4894-8071-27b78829327d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It \\' s the last he painted , you know , \" Mrs . Gisburn said with pardonable pride .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_text = token_transformer.decode(text_id)\n",
    "retrieved_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9187def7-7d94-4007-a304-b0d7e8c583a7",
   "metadata": {},
   "source": [
    "**Regarding:**\n",
    "\n",
    "> Note that it is possible to encounter a word that is completely new and hence has no token_id in our vocabulary\n",
    "\n",
    "The author pointed that out, and provided two notes: <br>\n",
    "(1) Using larger data set will mitigate this issue <br>\n",
    "(2) Even with (1), we may face some new words. In such case, some special tokens can be defined to handle those cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07de1fd3-5cd4-4cda-86a3-acafed3f2460",
   "metadata": {},
   "source": [
    "To address (2), the class `TokenTransform` is enhanced, and its new version is added to core.py as `TokenTransformV2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0a5957d-f80f-4d5d-aaca-9d5afeb23092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[95, 11, 586, 11]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get vocabulary using text file\n",
    "file_path = \"./data/the-verdict.txt\"\n",
    "raw_text = core.read_file(file_path)\n",
    "\n",
    "regex_pattern = r'([,.:;?_!\"()\\']|--|\\s)'\n",
    "tokens = core.tokenizer(raw_text, regex_pattern, remove_white_spaces=True)\n",
    "\n",
    "unique_tokens = sorted(set(tokens))\n",
    "vocabulary = {\n",
    "    token: token_id \n",
    "    for token_id, token in enumerate(unique_tokens)\n",
    "}\n",
    "\n",
    "# Passing vocabulary to token_transformer\n",
    "token_transformer = core.TokenTransformV2(vocabulary, regex_pattern)\n",
    "\n",
    "text = \"The sky is blue\"\n",
    "token_ids = token_transformer.encode(text)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b403cda-bca7-4ce9-a0af-4ffb8e2a4b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The <|unk|> is <|unk|>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_transformer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72c04bb-d37a-4ff7-ba02-c65189049158",
   "metadata": {},
   "source": [
    "## The tokenizer in GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035e1ff9-31d5-4518-bd11-029fb0b5cbde",
   "metadata": {},
   "source": [
    "In GPT, the tokenizer does not use a special token for out-of-vocabulary tokens. Instead, it uses `Byte Pair Encoding (BPE)`. To better understand the BPE, the author suggested to use the one that is already-implemented in [tiktoken](https://github.com/openai/tiktoken)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83585264-24c2-4100-82c2-9eb8ce039bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/nimasarajpoor/miniconda3/envs/stumpysdp/lib/python3.13/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/nimasarajpoor/miniconda3/envs/stumpysdp/lib/python3.13/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/nimasarajpoor/miniconda3/envs/stumpysdp/lib/python3.13/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/nimasarajpoor/miniconda3/envs/stumpysdp/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nimasarajpoor/miniconda3/envs/stumpysdp/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nimasarajpoor/miniconda3/envs/stumpysdp/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nimasarajpoor/miniconda3/envs/stumpysdp/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install tiktoken\n",
    "\n",
    "import tiktoken\n",
    "tiktoken.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13c9c152-dfc3-48f0-9128-a4dc803c6a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_id:  [43559, 318, 257, 649, 6827]\n",
      "==================================================\n",
      "token:  THIS is a new sentence\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"THIS is a new sentence\"\n",
    "\n",
    "token_id = tokenizer.encode(text)\n",
    "print('token_id: ', token_id)\n",
    "print('=' * 50)\n",
    "\n",
    "token = tokenizer.decode(token_id)\n",
    "print('token: ', token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77f0bcea-2b13-4ce6-933f-26ac3b614526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids:  [32, 6827, 266, 271, 4285, 31, 77, 469, 1195, 283, 19858]\n",
      "==================================================\n",
      "tokens:  A sentence wis Str@nge Qaracters\n"
     ]
    }
   ],
   "source": [
    "text = \"A sentence wis Str@nge Qaracters\"\n",
    "\n",
    "token_ids = tokenizer.encode(text)\n",
    "print('token_ids: ', token_ids)\n",
    "print('=' * 50)\n",
    "\n",
    "tokens = tokenizer.decode(token_ids)\n",
    "print('tokens: ', tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad01b9-2d65-4389-8be1-482ee87c3a9f",
   "metadata": {},
   "source": [
    "Note that the number of `token_ids` are more than the number of words. Let's see what each `token_id` is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2c2e96c-1f9e-48bd-bdb3-7fa9a8f740dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_id --> token\n",
      "--------------------------------------------------\n",
      "32 --> A\n",
      "6827 -->  sentence\n",
      "266 -->  w\n",
      "271 --> is\n",
      "4285 -->  Str\n",
      "31 --> @\n",
      "77 --> n\n",
      "469 --> ge\n",
      "1195 -->  Q\n",
      "283 --> ar\n",
      "19858 --> acters\n"
     ]
    }
   ],
   "source": [
    "print('token_id --> token')\n",
    "print('-' * 50)\n",
    "for token_id in token_ids:\n",
    "    print(f'{token_id} --> {tokenizer.decode([token_id])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a89ae0ae-c7e3-4dca-8c38-44f721c6d3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All token_ids: \n",
      " [33901, 86, 343, 86, 220, 959]\n",
      "==================================================\n",
      "token_id --> text\n",
      "33901 --> `Ak`\n",
      "86 --> `w`\n",
      "343 --> `ir`\n",
      "86 --> `w`\n",
      "220 --> ` `\n",
      "959 --> `ier`\n"
     ]
    }
   ],
   "source": [
    "text = \"Akwirw ier\"\n",
    "token_ids = tokenizer.encode(text)\n",
    "print('All token_ids: \\n', token_ids)\n",
    "print('=' * 50)\n",
    "\n",
    "print('token_id --> text')\n",
    "for token_id in token_ids:\n",
    "    token_text = tokenizer.decode([token_id])\n",
    "    print(f'{token_id} --> `{token_text}`')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c34dc0f-b457-44e7-824b-3bdaa2461bd0",
   "metadata": {},
   "source": [
    "## Data sampling with sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76bbbf6-935b-4528-ab75-68d122ea97b4",
   "metadata": {},
   "source": [
    "Suppose I have a sentence like this:\n",
    "\n",
    "`This is my sentence` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1fc6a1-c353-4437-a206-4c6ecd72bbfb",
   "metadata": {},
   "source": [
    "We can say: <br>\n",
    "* input: `[This, is, my, sentence]`\n",
    "* output: `[is, my, sentene, <END-OF-SENTENCE>]`\n",
    "\n",
    "Then, LLM will be trained on the following (input, output) pairs:\n",
    "\n",
    "* `This` --> `is`\n",
    "* `This is` --> `my`\n",
    "* `This is my` --> `sentence`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a030a7a6-2aa0-454c-b287-d5026e37c40a",
   "metadata": {},
   "source": [
    "The interesting point made by author is that the pytorch's tensor will look that input/output lists. For instance, if my text is:\n",
    "\n",
    "`This is the sentence in which I am providing some valuable information.`\n",
    "\n",
    "and if the window size is set to four, then:\n",
    "\n",
    "```\n",
    "inputs = [\n",
    "[This, is, the, sentence],\n",
    "[in, which, I, am],\n",
    "[providing, some, valuable, information],\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "[is, the, sentence, in],\n",
    "[which, I, am, providing],\n",
    "[some, valuable, information, <END-OF-TEXT],\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01baacdc-9209-48f3-8a64-06871644a345",
   "metadata": {},
   "source": [
    "**NOTE1:**\n",
    "IIUC, at some stage (?), LLM gets several (input, output) pairs from `inputs[0]` & `outputs[0]`. As mentioned above, those pairs are:\n",
    "* `This` --> `is`\n",
    "* `This is` --> `the`\n",
    "* `This is the` --> `sentence`\n",
    "* `This is the sentence` --> `in`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83364612-906a-4033-910b-d408427bcf7e",
   "metadata": {},
   "source": [
    "**NOTE2:** It is important to note that the list of inputs was created by sliding window 4 units. However, one can change it to a different number. For instance, sliding window by 1 unit gives the following:\n",
    "\n",
    "```\n",
    "inputs = [\n",
    "[This, is, the, sentence],\n",
    "[is, the, sentence, in],\n",
    "[the, sentence, in, which],\n",
    "[sentence, in, which, I],\n",
    "[in, which, I, am],\n",
    "[which, I, am, providing],\n",
    "[I, am, providing, some],\n",
    "[am, providing, some, valuable],\n",
    "[providing, some, valuable, information],\n",
    "]\n",
    "```\n",
    "\n",
    "And the outputs can be obtained accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b908e-1b0d-4328-890c-b8da0ee5a530",
   "metadata": {},
   "source": [
    "OK..I think it is time for me to stop and check out the appendix A of the book that provides some info about PyTorch. Then, I will come back to resume!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e25e4-b150-4b4c-9028-d9c754c7d412",
   "metadata": {},
   "source": [
    "## Prep data for GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97f0e4a5-148a-43d7-8c5e-78052f9b2180",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (3340218526.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31m_IncompleteInputError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTdataV1(dataset):\n",
    "    def __init__(self, txt)\n",
    "        self.__init__().super()\n",
    "        self.input_token_id = \n",
    "        self.target_token_id = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6284c7-64d1-4595-8aa9-7808703ad59f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
