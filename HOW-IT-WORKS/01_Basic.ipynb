{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9627d7d6-e7b8-41e5-9ec9-99ab789b31d4",
   "metadata": {},
   "source": [
    "**NOTE:** The following information is based on the book \"Build Large Language Model From Scratch\" By Sebastian Raschka. I am just trying to take notes, explain some stuff further for myself when needed, and do some coding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902ccc0a-fb13-4113-95cd-856c9fd7696e",
   "metadata": {},
   "source": [
    "The following figure from the book is really nice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e540ad41-7712-4ebd-8899-1c63b2dd2ba3",
   "metadata": {},
   "source": [
    "![end-to-end-LLM](./images/end-to-end-LLM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c90548a-bf73-4fea-9b83-6dc959784071",
   "metadata": {},
   "source": [
    "At the end of the day, what we are looking for is to get something like this:\n",
    "\n",
    "> user input --> MODEL --> output\n",
    "\n",
    "\n",
    "How to get that MODEL?\n",
    "\n",
    "* Stage 1: Prepare training data, and Design (Create) LLM's architecture\n",
    "* Stage1 -> Stage 2:  Pre-training LLM, i.e. using general data to get a fitted model (known as foundational model) for general-purpose tasks.\n",
    "* Stage 2: Evaluating model. Also, revisit `Stage1 -> Stage2` and use publicly available pre-trained weights\n",
    "* Stage 2 --> 3: Fine-tuning model using task/domain-specific data. They can perform better (e.g. BloombergGPT)\n",
    "* Stage 3: We now have Classifier (which uses class labels) OR Personal Assistant (which uses instruction dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1febd0a9-3275-495a-879e-87f7c9ee0eb1",
   "metadata": {},
   "source": [
    "As mentioned above, having custom-made LLM performs better when it needs to be used for a particular task or domain. Furthermore, it allows to get same performance with smaller model, which makes it possible to be embedded on user's system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d625272f-fa20-4caf-b39d-9e2fd0cc55df",
   "metadata": {},
   "source": [
    "A GPT-like model, at its core, is based on transformers. In contrast to transformers that contain both encoding and decoding layers, the GPT's architecture only contains the decoder part. The model is mainly trained to predict the next word (token). Therefore, in Stage 1, the training data does NOT need to have label! Because, we can get it from the text data itself. For instance, if there is a sentence like: `The sky is blue`. I can just get the last word of the sentence and use it as the \"label\" for the sample `The sky is`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b2535d-90e6-49fc-978d-2c96100e98c2",
   "metadata": {},
   "source": [
    "**NOTE: fine-tuning** <br>\n",
    "The two common ways of fine-tuning are: (i) instruction fine-tuning, (ii) classification fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651a679c-672d-47d5-b2d8-1fe78324c0d1",
   "metadata": {},
   "source": [
    "**NOTE: self-attention** <br>\n",
    "One of the key components of transformers that play an important role in LLM is `self-attention`. It somehow gives weigh to words to show their importance relative to each other! So, it can take an input with longer length as it can pay better attention to the parts that matter for predicting the next word in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81052f1-7322-4841-a150-fbf6f6eaad9b",
   "metadata": {},
   "source": [
    "**NOTE: GPT vs BERT** <br>\n",
    "`GPT` is mainly designed to perform \"text completion\" by predicting the next word in the sequence. `BERT`, however, is better at \"masked word prediction\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dfed1f-9a09-432b-a502-8a45ce3ceb8d",
   "metadata": {},
   "source": [
    "**Note:** <br>\n",
    "As mentioned earlier, the transformer layer in GPT has the decoder part only, and it helps with predicting the next word. The cool thing is that the output is then used as the input for predicting the word after that, and so on! Hence, the GPT model is known to be of type `autoregressive` model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc98e4-a145-4890-93e7-f5e51dd2048c",
   "metadata": {},
   "source": [
    "**NOTE: When to use GPT?** <br>\n",
    "GPT-like model is good at predicing next word, and text generation in general. So, the following tasks can get benefit from it:\n",
    "* Machine Translation\n",
    "* Text Summarization\n",
    "* Writing new articles / code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6670226c-53f8-4b8d-bbfd-dcaaa2416a01",
   "metadata": {},
   "source": [
    "**NOTE: Different ways to leverage GPT** <br>\n",
    "* As is: text completion!\n",
    "* Zero-shot: Perform task WITHOUT any domain/task-specific training data (Example: `Translate to Farsi: Sky -->`)\n",
    "* Few-shot: Perform task with a few examples (Example: `woc -> cow, sky --> yks, lie --> eil, nima --> `)\n",
    "\n",
    "And, of course, some other ways to use it is: \n",
    "* fine-tuning it via instruction\n",
    "* fine-tuning it via classification labels.\n",
    "\n",
    "**Question:** But...how does the model understand the \"instruction\" from input? and seperate it from the input whose output is being expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e73268c-4f6b-4388-b95f-96a00a1ed807",
   "metadata": {},
   "source": [
    "One of the cool thing the book has pointed out is that the GPT-like model can perform well for translation despite the fact that it is just decoder-only and it is mainly designed to predict the next word (and NOT translation). It is called \"emergent behaviour\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c1716-d767-4e56-b784-441a2bfe4ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
